# scrapy_project

personal project of a future website that consists of a crawler that searches for data from Amazon and stores it in an SQL database.

## Technologies

- Python 3.9+
- Scrapy
- PostgreSQL

## Structure

scrapy_project/
â”œâ”€â”€ book_data_analysis/ # Project root
â”‚ â”œâ”€â”€ init.py # Init file for module
â”‚ â”œâ”€â”€ items.py # Item definitions for Scrapy
â”‚ â”œâ”€â”€ middlewares.py # Custom middleware (if any)
â”‚ â”œâ”€â”€ pipelines.py # Pipelines for processing scraped data
â”‚ â”œâ”€â”€ settings.py # Scrapy settings
â”‚ â””â”€â”€ spiders/ # Spider definitions
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ amazonLink_spider.py # Main spider for Amazon book links
â”œâ”€â”€ scrapy.cfg # Scrapy config file
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # This file

## Installation

```bash
git clone https://github.com/MarvynMadeira/scrapy_project.git
cd scrapy_project
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt

## Running the Spider

scrapy crawl amazonLink_spider -a url=""

## Data Analysis
Use the scripts inside the book_data_analysis/spiders/ folder (e.g., amazonLink_spider.py) to define how the spider extracts data from Amazon.

ðŸ“¬ Contact
https://github.com/MarvynMadeira
